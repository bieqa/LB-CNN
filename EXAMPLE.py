"""
An example of LB-CNN/graph-CNN using different kinds of polynomial
approximations [1] (default: LB-CNN, Chebyshev approximation, 3 convolutional layer).
This example was created based on the code given by [2] in 
(https://bieqa.github.io/deeplearning.html).

----------------------------------------------------------------
Input data: Need to provide the following input data in advance
    ./data/data.m: surface data, including 
                    'data_test'         dimension x (testing size); 
                    'data_trainvalid'   dimension x (train+validation size); 
                    'label_test'        1 x (testing size), value=0 or 1;
                    'label_trainvalid'  1 x (train+validation size), value=0 or 1.

    ./data/adjacency_LB.mat or adjacency_gL.mat: 
                    sparse adjacency matrix of LB-operator or graph Laplacian,
                    which can be generated by the provided MATLAB package

Output results: 
    ./result/?/performance_test.csv: 
                    testing performance (accuracy, sensitivity, specificity, ...)    

    ./result/?/label_test.csv: 
                    predicted labels and probabilities of testing set

    ./result/?/performance_trainvalid.csv:
                    training and validation performance
    
    other output files are saved for future use (eg. transfer learning) 
----------------------------------------------------------------

Reference:
[1] Huang S.-G., Chung, M.K. and Qiu, A. Revisiting convolutional neural network 
on graphs with polynomial approximations of Laplace-Beltrami spectral filtering. 
Under review.

[2] Liu, C., Ji, H. and Qiu, A. Convolutional Neural Network on Semi-Regular 
Triangulated Meshes and its Application to Brain Image Data. arXiv preprint 
arXiv:1903.08828, 2019.


Oct. 20, 2020  Created by Shih-Gu Huang

"""

from lib import coarsening
from lib import graph_LB as graph
from lib import models_cnn as models
from lib import helper_func as hf

import numpy as np
from scipy import sparse
from sklearn import preprocessing
import pandas as pd

import h5py
import math
import os
import time
import pickle
import datetime


os.environ["CUDA_VISIBLE_DEVICES"]="1" 


params = dict()

# ----- Architecture ----------------------------------------------------------
params['filter']    = 'chebyshev'   # 'chebyshev', 'laguerre' or 'hermite'  approximation
Laplace='LB'                        # 'LB': LB-operator;  'gL': graph Laplacian
params['algo']      = Laplace
params['normalized']= False         # unormalized or normalized Laplacian 
                                    # normalization may make LB-CNN unstable (failing to converge sometimes)
params['pool']      = 'apool1'      # 'apool1': average pooling;  'mpool1': max pooling

## 3 convolutional layers
params['F']   = [8, 16, 32]         # number of convolutional filters
params['K']   = [7, 7 ,7]           # polynomial approximation orders
params['p']   = [16, 8, 4]          # pooling sizes
params['M']   = [128, 2]            # 128 hidden nodes, 2 classes 
coarsen_level = int(math.log(np.sum(np.prod(params['p'])), 2))  # levels of graph coarsening (dependent of pooling size)



# ----- Hyper-parameters ------------------------------------------------------
params['num_epochs']     = 30       # number of epochs
params['batch_size']     = 32       # batch size
params['learning_rate']  = 1e-3     # initial learning rate
params['decay_rate']     = 1-0.05   # decay of learning rate
params['decay_steps']    = 20       
params['momentum']       = 0.9      # momentum
params['dropout']        = 1.       # no droupout
params['regularization'] = 5e-4     # regularization in FC layer
params['eval_frequency'] = 20       # number of iterations to display results on workspace 
valid_ratio = 0.25                  # ratio of validation data



# ----- data and result folders -----------------------------------------------
DATADIR = 'data'                            # data folder
Datafilename='data.mat'                     # data file
Adjfilename='adjacency_' + Laplace + '.mat' # adjacency_LB.mat  or  adjacency_gL.mat
SAVEDIR = 'result'                          # result folder



# ----- paths of data and result files ----------------------------------------
path = os.path.dirname(os.path.realpath(__file__))
Datafilename=os.path.join(path, DATADIR, Datafilename)                  # data file path
Adjfilename=os.path.join(path, DATADIR, Adjfilename)                    # adjacency matrix file path

szDatetime = datetime.datetime.now().strftime('%s')
szDatetime = Laplace + '_' + params['filter'] + '_' + szDatetime
save_dir = os.path.join(path, SAVEDIR, szDatetime)                      # result sub-folder
params['dir_name']       = save_dir

Labelfilename   = os.path.join(save_dir , 'label_test.csv')             # prediction file path
Accuaryfilename = os.path.join(save_dir , 'performance_trainvalid.csv') # performance (train/valid) file path
Resultfilename  = os.path.join(save_dir , 'performance_test.csv')       # performance (testing) file path



# ----- Load data and labels --------------------------------------------------
print('Loading data ...')
f = h5py.File(Datafilename, 'r')
print(list(f.keys()),'\n')

data_trainvalid = np.array(f.get('data_trainvalid')).astype(float)    
data_test = np.array(f.get('data_test')).astype(float)
target_trainvalid = np.transpose(np.array(f.get('label_trainvalid')).astype(int))[0]
target_test = np.transpose(np.array(f.get('label_test')).astype(int))[0]


all_start_time = time.time()


# ----- remove mean and scale to unit variance --------------------------------
X_train = np.copy(data_trainvalid)
X_test = np.copy(data_test)
scaler = preprocessing.StandardScaler().fit(X_train)
data_trainvalid = scaler.transform(X_train)
data_test = scaler.transform(X_test)



# ----- Create training and validation sets -----------------------------------
print('Data splitting...')
target_trainvalid = np.transpose(np.vstack([np.transpose(1-target_trainvalid), np.transpose(target_trainvalid)]))

data_train, target_train, data_valid, target_valid = hf.train_valid_data(data_trainvalid, target_trainvalid, valid_ratio)
data_train = data_train.astype(np.float32)
data_valid = data_valid.astype(np.float32)
data_test = data_test.astype(np.float32)
del data_trainvalid


target_train = np.copy(target_train[:,1])
target_valid = np.copy(target_valid[:,1])    
target_train = target_train.astype(np.uint8)
target_valid = target_valid.astype(np.uint8)
target_test  = target_test.astype(np.uint8)

print('Size of training set: ', data_train.shape)
print('Size of validation set: ', data_valid.shape)
print('Size of testing set: ', data_test.shape)

print('Group ratio in training set: ', len(target_train)-sum(target_train),':',sum(target_train))
print('Group ratio in validation set: ',len(target_valid)-sum(target_valid),':',sum(target_valid))
print('Group ratio in testing set: ',   len(target_test)-sum(target_test),':',sum(target_test),'\n')


      
# ----- Load adjacency matrix of LB-operator or graph Laplacian ---------------
# L = D-W
print('Loading adjacency matrix ...')
f_adjacency = h5py.File(Adjfilename, 'r')   
W = sparse.csr_matrix((f_adjacency["W"]["data"], f_adjacency["W"]["ir"], f_adjacency["W"]["jc"]))
W = W.astype(np.float32)
print('Size of W: ', W.shape,'\n')


# ----- Graph coarsening ------------------------------------------------------
print('Graph coarsening  ...')
print('Original: |V| = {} nodes, |E| = {} edges'.format(W.shape[0], int(W.nnz/3)))
graphs, perm = coarsening.coarsen(W, levels=coarsen_level, self_connections=False)

# exchange node ids so that binary unions form the clustering tree.
data_train = coarsening.perm_data(data_train, perm)
data_valid = coarsening.perm_data(data_valid, perm)
data_test = coarsening.perm_data(data_test, perm)



# ----- Update LB-operator or graph Laplacian for each coarsened level --------
L = [graph.laplacian(W2, normalized=params['normalized']).transpose() for W2 in graphs]    



# ----- Training and validation -----------------------------------------------
n_train = data_train.shape[0]       # Number of train samples.
params['decay_steps']    = (np.multiply(params['decay_steps'], n_train / params['batch_size'])).astype(int)
model = models.cgcnn(L, 1, **params)

val_loss, val_accuracy, val_fscore, val_sensitivity, val_specificity, val_precision, val_ppv, val_npv, val_gmean \
= model.fit(data_train, target_train, data_valid, target_valid, params['learning_rate'],params['num_epochs'],params['decay_steps'])

# Store the generated Laplacian matrix, graphs, permutation index for future use (transfer learning)
with open(save_dir+'/laplacian_graphs.pkl', 'wb') as f2:
    pickle.dump([W, L, graphs, perm, scaler], f2)  

# training time
all_time = float(time.time() - all_start_time) 
print ('whole time: %.3f, epoch time: %.3f' % (all_time, all_time/params['num_epochs']),'\n')

 

# ----- Evaluate performance on testing set -----------------------------------
prevalence = np.sum(target_test)*1.0 / target_test.shape[0]
res, loss, accuracy, f1, sensitivity, specificity, precision, ppv, npv, gmean, predictions, logits = model.evaluate(data_test, target_test)   

# print and save performance
hf.print_classification_performance(prevalence, loss, accuracy, f1, sensitivity, specificity, precision, ppv, npv, gmean)
hf.save_to_csv(szDatetime, accuracy, f1, sensitivity, specificity, precision, ppv, npv,\
               gmean, Resultfilename, all_time, all_time/params['num_epochs'], mode='a+', header=1)    

# save prediction results (including probabilities) 
probabilities = hf.softmax(np.transpose(np.array(logits)))  # probabilities of the two classes
d= {'Actual':target_test, 'Predicted':predictions,'Probability0':probabilities[0,:] ,'Probability1':probabilities[1,:]}
d = pd.DataFrame(data=d, dtype=np.float)    
d.to_csv(Labelfilename)



del model



